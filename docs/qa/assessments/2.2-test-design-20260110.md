# Test Design: Story 2.2 - Bounds Filtering

Date: 2026-01-10
Designer: Quinn (Test Architect)

## Test Strategy Overview

| Metric | Count | Percentage |
|--------|-------|------------|
| Total test scenarios | 28 | 100% |
| Unit tests | 12 | 43% |
| Widget tests | 10 | 36% |
| Integration tests | 5 | 18% |
| Performance tests | 1 | 3% |

| Priority | Count |
|----------|-------|
| P0 | 8 |
| P1 | 12 |
| P2 | 8 |

## Test Scenarios by Acceptance Criteria

---

### AC1: "Add Filter" button with filter row (column, operator, min, max, remove)

**Requirement**: User can add filter rows with all required UI elements

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-WIDG-001 | Widget | P1 | "Add Filter" button creates new FilterRow | UI component behavior |
| 2.2-WIDG-002 | Widget | P1 | FilterRow contains column dropdown | UI structure verification |
| 2.2-WIDG-003 | Widget | P1 | FilterRow contains operator dropdown | UI structure verification |
| 2.2-WIDG-004 | Widget | P1 | FilterRow contains min/max inputs | UI structure verification |
| 2.2-WIDG-005 | Widget | P2 | Remove button removes FilterRow | UI interaction |
| 2.2-WIDG-006 | Widget | P2 | Column dropdown populates with numeric columns | Data binding |
| 2.2-WIDG-007 | Widget | P2 | Max 10 filters enforced (Limits.MAX_FILTERS) | Business rule in UI |

**Test File**: `tests/widget/test_filter_panel.py`

---

### AC2: Operators: "between", "not between"

**Requirement**: Two filter operators available with correct behavior

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-UNIT-001 | Unit | P0 | `between` operator returns correct boolean mask | Pure logic - critical filter behavior |
| 2.2-UNIT-002 | Unit | P0 | `not_between` operator returns correct boolean mask | Pure logic - critical filter behavior |
| 2.2-UNIT-003 | Unit | P1 | `between` includes boundary values (inclusive) | Edge case - boundary behavior |
| 2.2-UNIT-004 | Unit | P1 | `not_between` excludes boundary values | Edge case - boundary behavior |
| 2.2-WIDG-008 | Widget | P1 | Operator dropdown shows "between" and "not between" | UI options verification |

**Test Files**: `tests/unit/test_filter_criteria.py`, `tests/widget/test_filter_row.py`

**Unit Test Examples**:
```python
def test_between_filter_includes_boundaries():
    """BETWEEN filter includes min and max values (inclusive)."""
    df = pd.DataFrame({"gain_pct": [0, 5, 10]})
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=10)
    mask = criteria.apply(df)
    assert mask.all()  # All values 0, 5, 10 should match

def test_not_between_filter_excludes_range():
    """NOT BETWEEN filter excludes values in range."""
    df = pd.DataFrame({"gain_pct": [-5, 5, 15]})
    criteria = FilterCriteria(column="gain_pct", operator="not_between", min_val=0, max_val=10)
    mask = criteria.apply(df)
    assert mask.tolist() == [True, False, True]  # -5 and 15 outside range
```

---

### AC3: Filter validation (min ≤ max, numeric values)

**Requirement**: Invalid filters are rejected with clear error messages

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-UNIT-005 | Unit | P0 | `validate()` returns error when min > max | Pure validation logic |
| 2.2-UNIT-006 | Unit | P1 | `validate()` returns None for valid criteria | Happy path validation |
| 2.2-WIDG-009 | Widget | P1 | FilterRow shows validation error for invalid input | UI feedback |
| 2.2-WIDG-010 | Widget | P2 | Non-numeric input rejected in min/max fields | Input validation |

**Test File**: `tests/unit/test_filter_criteria.py`

**Unit Test Examples**:
```python
def test_validate_returns_error_when_min_greater_than_max():
    """validate() returns error when min > max."""
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=10, max_val=5)
    error = criteria.validate()
    assert error is not None
    assert "Min" in error or "min" in error.lower()

def test_validate_returns_none_for_valid_criteria():
    """validate() returns None for valid filter criteria."""
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=10)
    assert criteria.validate() is None
```

---

### AC4: "Apply Filters" updates chart and row count

**Requirement**: Filter application triggers chart and count updates

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-UNIT-007 | Unit | P0 | FilterEngine applies single filter correctly | Core engine logic |
| 2.2-UNIT-008 | Unit | P0 | FilterEngine applies multiple filters with AND logic | Core engine logic |
| 2.2-UNIT-009 | Unit | P1 | FilterEngine returns original DataFrame for empty filter list | Edge case |
| 2.2-UNIT-010 | Unit | P0 | FilterEngine returns copy, not view | Data safety - critical |
| 2.2-INT-001 | Integration | P0 | Apply filters → AppState.filtered_df updated | State management |
| 2.2-INT-002 | Integration | P1 | Apply filters → filtered_data_updated signal emitted | Signal chain |
| 2.2-INT-003 | Integration | P1 | Chart updates when filters applied | Full workflow |

**Test Files**: `tests/unit/test_filter_engine.py`, `tests/integration/test_filter_workflow.py`

**Unit Test Examples**:
```python
def test_apply_filters_and_logic(sample_trades):
    """Multiple filters combine with AND logic."""
    engine = FilterEngine()
    filters = [
        FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=10),
        FilterCriteria(column="gain_pct", operator="between", min_val=5, max_val=15),
    ]
    result = engine.apply_filters(sample_trades, filters)
    # Only values between 5 and 10 should match (intersection)
    assert (result["gain_pct"] >= 5).all()
    assert (result["gain_pct"] <= 10).all()

def test_apply_filters_returns_copy(sample_trades):
    """Filtered DataFrame is a copy, not a view."""
    engine = FilterEngine()
    filters = [FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=5)]
    result = engine.apply_filters(sample_trades, filters)
    # Modifying result should not affect original
    original_values = sample_trades["gain_pct"].copy()
    result["gain_pct"] = 999
    assert (sample_trades["gain_pct"] == original_values).all()
```

**Integration Test Example**:
```python
def test_filter_workflow_updates_chart(qtbot, app_state, sample_trades):
    """Full workflow: apply filters → chart updates with filtered data."""
    # Setup
    app_state.baseline_df = sample_trades
    tab = FeatureExplorerTab(app_state)
    qtbot.addWidget(tab)
    
    # Create and apply filter
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=5)
    
    with qtbot.waitSignal(app_state.filtered_data_updated) as blocker:
        # Simulate filter application
        tab._on_filters_applied([criteria])
    
    # Verify
    assert app_state.filtered_df is not None
    assert len(app_state.filtered_df) <= len(sample_trades)
```

---

### AC5: Applied filters shown as styled chips (nova-amber)

**Requirement**: Active filters displayed as removable chips with amber styling

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-WIDG-011 | Widget | P1 | FilterChip displays filter summary correctly | UI display |
| 2.2-WIDG-012 | Widget | P1 | FilterChip remove button emits `removed` signal | UI interaction |
| 2.2-WIDG-013 | Widget | P2 | FilterChip styled with amber background | Visual styling |
| 2.2-WIDG-014 | Widget | P2 | Chips appear in FilterPanel after filter applied | UI state |

**Test File**: `tests/widget/test_filter_chip.py`

**Widget Test Examples**:
```python
def test_filter_chip_displays_summary(qtbot):
    """FilterChip shows filter criteria summary."""
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=10)
    chip = FilterChip(criteria)
    qtbot.addWidget(chip)
    
    label_text = chip._label.text()
    assert "gain_pct" in label_text
    assert "between" in label_text
    assert "0" in label_text
    assert "10" in label_text

def test_filter_chip_remove_emits_signal(qtbot):
    """Remove button emits signal with criteria."""
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=10)
    chip = FilterChip(criteria)
    qtbot.addWidget(chip)
    
    with qtbot.waitSignal(chip.removed) as blocker:
        chip._remove_btn.click()
    
    assert blocker.args[0] == criteria
```

---

### AC6: "Clear All Filters" resets to baseline

**Requirement**: Clear action removes all filters and restores baseline data

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-WIDG-015 | Widget | P1 | "Clear All Filters" removes all FilterRows | UI state reset |
| 2.2-WIDG-016 | Widget | P1 | "Clear All Filters" emits `filters_cleared` signal | Signal emission |
| 2.2-INT-004 | Integration | P1 | Clear filters → AppState.filtered_df = baseline_df | State restoration |
| 2.2-INT-005 | Integration | P2 | Clear filters → chart shows baseline data | Full workflow |

**Test Files**: `tests/widget/test_filter_panel.py`, `tests/integration/test_filter_workflow.py`

**Integration Test Example**:
```python
def test_clear_filters_restores_baseline(qtbot, app_state, sample_trades):
    """Clear filters restores baseline data."""
    # Setup with filters applied
    app_state.baseline_df = sample_trades
    app_state.filters = [FilterCriteria(column="gain_pct", operator="between", min_val=0, max_val=5)]
    app_state.filtered_df = sample_trades[sample_trades["gain_pct"] <= 5].copy()
    
    tab = FeatureExplorerTab(app_state)
    qtbot.addWidget(tab)
    
    # Clear filters
    tab._on_filters_cleared()
    
    # Verify baseline restored
    assert len(app_state.filtered_df) == len(app_state.baseline_df)
    assert app_state.filters == []
```

---

### AC7: Filter applied in < 500ms for 100k rows

**Requirement**: Performance target for large datasets

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 2.2-PERF-001 | Performance | P0 | Filter application < 500ms for 100k rows | NFR - performance SLA |

**Test File**: `tests/integration/test_performance.py`

**Performance Test Example**:
```python
@pytest.mark.slow
def test_filter_response_under_500ms():
    """NFR: Filter application < 500ms for 100k rows."""
    from time import perf_counter
    import numpy as np
    
    # Generate 100k row dataset
    np.random.seed(42)
    df = pd.DataFrame({
        "gain_pct": np.random.uniform(-10, 10, 100_000),
        "ticker": ["TEST"] * 100_000,
    })
    
    engine = FilterEngine()
    criteria = FilterCriteria(column="gain_pct", operator="between", min_val=-5, max_val=5)
    
    start = perf_counter()
    result = engine.apply_filters(df, [criteria])
    elapsed = perf_counter() - start
    
    assert elapsed < 0.5, f"Filter took {elapsed:.3f}s, exceeds 500ms limit"
    assert len(result) > 0  # Sanity check
```

---

## Risk Coverage Matrix

| Risk ID | Risk Description | Mitigating Tests |
|---------|------------------|------------------|
| TECH-001 | Signal chain management | 2.2-INT-001, 2.2-INT-002, 2.2-INT-003 |
| DATA-001 | DataFrame view vs copy | 2.2-UNIT-010 |
| TECH-002 | Widget lifecycle | 2.2-WIDG-005, 2.2-WIDG-015 |
| BUS-001 | UI responsiveness | 2.2-PERF-001 |
| DATA-002 | Filter state inconsistency | 2.2-INT-001, 2.2-INT-004 |
| PERF-001 | 100k row performance | 2.2-PERF-001 |

---

## Test File Structure

```
tests/
├── unit/
│   ├── test_filter_criteria.py     # 2.2-UNIT-001 to 2.2-UNIT-006
│   └── test_filter_engine.py       # 2.2-UNIT-007 to 2.2-UNIT-010
├── widget/
│   ├── test_filter_chip.py         # 2.2-WIDG-011 to 2.2-WIDG-014
│   ├── test_filter_row.py          # 2.2-WIDG-008 to 2.2-WIDG-010
│   └── test_filter_panel.py        # 2.2-WIDG-001 to 2.2-WIDG-007, 2.2-WIDG-015, 2.2-WIDG-016
└── integration/
    ├── test_filter_workflow.py     # 2.2-INT-001 to 2.2-INT-005
    └── test_performance.py         # 2.2-PERF-001
```

---

## Recommended Execution Order

### Phase 1: P0 Tests (Fail Fast)
1. `test_filter_criteria.py` - 2.2-UNIT-001, 2.2-UNIT-002, 2.2-UNIT-005
2. `test_filter_engine.py` - 2.2-UNIT-007, 2.2-UNIT-008, 2.2-UNIT-010
3. `test_filter_workflow.py` - 2.2-INT-001
4. `test_performance.py` - 2.2-PERF-001

### Phase 2: P1 Tests (Core Functionality)
5. Remaining unit tests (2.2-UNIT-003, 2.2-UNIT-004, 2.2-UNIT-006, 2.2-UNIT-009)
6. Widget tests for core components (2.2-WIDG-001 to 2.2-WIDG-004, 2.2-WIDG-008, 2.2-WIDG-009)
7. Filter chip tests (2.2-WIDG-011, 2.2-WIDG-012)
8. Clear filters tests (2.2-WIDG-015, 2.2-WIDG-016, 2.2-INT-004)
9. Signal chain tests (2.2-INT-002, 2.2-INT-003)

### Phase 3: P2 Tests (Polish)
10. Remaining widget tests (2.2-WIDG-005 to 2.2-WIDG-007, 2.2-WIDG-010, 2.2-WIDG-013, 2.2-WIDG-014)
11. Clear filters integration (2.2-INT-005)

---

## Fixture Requirements

### conftest.py additions

```python
@pytest.fixture
def sample_trades():
    """Sample trade data for filter testing."""
    return pd.DataFrame({
        "gain_pct": [-5.0, -2.5, 0.0, 2.5, 5.0, 7.5, 10.0],
        "ticker": ["AAPL", "GOOGL", "MSFT", "AMZN", "META", "NVDA", "TSLA"],
        "date": pd.date_range("2025-01-01", periods=7),
    })

@pytest.fixture
def sample_filter_criteria():
    """Sample filter criteria for testing."""
    return FilterCriteria(
        column="gain_pct",
        operator="between",
        min_val=0.0,
        max_val=5.0,
    )

@pytest.fixture
def large_dataset():
    """100k row dataset for performance testing."""
    import numpy as np
    np.random.seed(42)
    return pd.DataFrame({
        "gain_pct": np.random.uniform(-10, 10, 100_000),
        "ticker": np.random.choice(["AAPL", "GOOGL", "MSFT"], 100_000),
    })
```

---

## Quality Checklist

- [x] Every AC has test coverage
- [x] Test levels are appropriate (unit for logic, widget for UI, integration for workflows)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk (P0 for core filter logic and performance)
- [x] Test IDs follow naming convention (2.2-{LEVEL}-{SEQ})
- [x] Scenarios are atomic and independent
- [x] Risk mitigations mapped to specific tests

---

## Gate YAML Block

```yaml
test_design:
  scenarios_total: 28
  by_level:
    unit: 12
    widget: 10
    integration: 5
    performance: 1
  by_priority:
    p0: 8
    p1: 12
    p2: 8
  coverage_gaps: []
```

---

Test design matrix: docs/qa/assessments/2.2-test-design-20260110.md
P0 tests identified: 8
