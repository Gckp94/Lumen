# Test Design: Story 4.1

Date: 2026-01-11
Designer: Quinn (Test Architect)

## Test Strategy Overview

- Total test scenarios: 24
- Unit tests: 6 (25%)
- Widget tests: 6 (25%)
- Integration tests: 9 (37.5%)
- Performance tests: 3 (12.5%)
- E2E tests: 0 (0%)
- Priority distribution: P0: 8, P1: 10, P2: 6

**Strategy Rationale:** This story involves state management, signal flows, and performance requirements. The test pyramid emphasizes integration tests (37.5%) because the core value is in component interactions (filter changes triggering calculations). Unit tests focus on edge cases in the metrics calculation. Widget tests validate the new CalculationStatusIndicator. Performance tests validate the critical <100ms and 300ms debounce requirements.

## Test Scenarios by Acceptance Criteria

### AC1: Trigger on filter change or tab navigation

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-INT-001 | Integration | P0 | Filter change triggers `filtered_data_updated` signal | Core trigger mechanism |
| 4.1-INT-002 | Integration | P0 | `filtered_data_updated` triggers metrics recalculation | Signal → calculation flow |
| 4.1-INT-003 | Integration | P1 | Tab navigation triggers recalculation when hash differs | Tab-based trigger |
| 4.1-INT-004 | Integration | P1 | Tab navigation skips recalculation when hash unchanged | Optimization check |

### AC2: Reuse Epic 3 metrics engine

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-INT-005 | Integration | P0 | `_calculate_filtered_metrics()` calls `MetricsCalculator.calculate()` | Engine reuse verification |
| 4.1-UNIT-001 | Unit | P1 | Filtered calculation produces same results as baseline for same data | Consistency check |

### AC3: Store filtered metrics and equity curves in app state

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-INT-006 | Integration | P0 | `filtered_metrics` stored in AppState after calculation | State storage |
| 4.1-INT-007 | Integration | P1 | `filtered_flat_stake_equity_curve` stored in AppState | Equity storage |
| 4.1-INT-008 | Integration | P1 | `filtered_kelly_equity_curve` stored in AppState | Equity storage |
| 4.1-INT-009 | Integration | P1 | `metrics_updated` signal emits both baseline and filtered | Signal emission |

### AC4: Performance: core stats < 100ms, equity curves with 300ms debounce

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-PERF-001 | Performance | P0 | Core stats calculation < 100ms with 10k rows | Performance requirement |
| 4.1-PERF-002 | Performance | P1 | Core stats calculation < 100ms with 100k rows | Stress test |
| 4.1-PERF-003 | Performance | P0 | Equity curve calculation debounced at 300ms | Debounce verification |

### AC5: Handle edge cases (no matches, single row)

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-UNIT-002 | Unit | P0 | Empty DataFrame returns `TradingMetrics.empty()` | Mitigates DATA-001 |
| 4.1-UNIT-003 | Unit | P0 | Single row returns valid metrics (std=None) | Mitigates DATA-002 |
| 4.1-UNIT-004 | Unit | P1 | Filtered subset produces expected metrics values | Correctness check |
| 4.1-UNIT-005 | Unit | P2 | All winners returns loser_* fields as None | Edge case |
| 4.1-UNIT-006 | Unit | P2 | All losers returns winner_* fields as None | Edge case |

### AC6: Calculation status indicator

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 4.1-WGT-001 | Widget | P0 | Indicator shows "Calculating..." when signal emitted | Status display |
| 4.1-WGT-002 | Widget | P0 | Indicator shows "Ready" after completion signal | Status display |
| 4.1-WGT-003 | Widget | P1 | Indicator styled with theme constants | Visual consistency |
| 4.1-WGT-004 | Widget | P2 | Indicator has subtle animation during calculation | UX polish |
| 4.1-WGT-005 | Widget | P2 | Indicator properly placed in PnL Stats tab | Layout verification |
| 4.1-WGT-006 | Widget | P2 | Indicator exported from components/__init__.py | Module structure |

## Risk Coverage Mapping

| Risk ID | Test IDs | Coverage |
|---------|----------|----------|
| PERF-001 | 4.1-PERF-001, 4.1-PERF-002 | Full |
| TECH-001 | 4.1-PERF-003, 4.1-INT-002 | Full |
| DATA-001 | 4.1-UNIT-002 | Full |
| DATA-002 | 4.1-UNIT-003 | Full |
| TECH-002 | 4.1-INT-003, 4.1-INT-004 | Full |
| TECH-003 | Not tested (minimal risk) | Accepted |
| TECH-004 | Code review | Accepted |

## Detailed Test Specifications

### Unit Tests (tests/unit/test_metrics.py)

```python
class TestFilteredMetricsEdgeCases:
    """Tests for filtered data calculation edge cases."""

    def test_calculate_empty_df_returns_empty_metrics(self):  # 4.1-UNIT-002
        """Empty DataFrame returns TradingMetrics.empty()."""
        calculator = MetricsCalculator()
        metrics, flat_eq, kelly_eq = calculator.calculate(
            df=pd.DataFrame(),
            gain_col="gain_pct",
        )
        assert metrics == TradingMetrics.empty()
        assert flat_eq is None
        assert kelly_eq is None

    def test_calculate_single_row(self):  # 4.1-UNIT-003
        """Single row returns valid metrics with some None fields."""
        calculator = MetricsCalculator()
        df = pd.DataFrame({"gain_pct": [5.0]})
        metrics, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        assert metrics.num_trades == 1
        assert metrics.avg_gain == 5.0
        assert metrics.std_gain is None  # Can't calculate std for 1 value

    def test_calculate_filtered_subset(self):  # 4.1-UNIT-004
        """Filtered subset produces expected metric values."""
        calculator = MetricsCalculator()
        df = pd.DataFrame({"gain_pct": [10.0, -5.0, 8.0, -3.0]})
        metrics, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        assert metrics.num_trades == 4
        assert metrics.num_winners == 2
        assert metrics.num_losers == 2

    def test_filtered_consistency_with_baseline(self):  # 4.1-UNIT-001
        """Filtered calculation matches baseline for same data."""
        calculator = MetricsCalculator()
        df = pd.DataFrame({"gain_pct": [5.0, -2.0, 3.0]})
        baseline, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        filtered, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        assert baseline == filtered

    def test_all_winners_loser_fields_none(self):  # 4.1-UNIT-005
        """All winning trades returns None for loser fields."""
        calculator = MetricsCalculator()
        df = pd.DataFrame({"gain_pct": [5.0, 3.0, 8.0]})
        metrics, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        assert metrics.num_losers == 0
        assert metrics.avg_loser is None

    def test_all_losers_winner_fields_none(self):  # 4.1-UNIT-006
        """All losing trades returns None for winner fields."""
        calculator = MetricsCalculator()
        df = pd.DataFrame({"gain_pct": [-5.0, -3.0, -8.0]})
        metrics, _, _ = calculator.calculate(df=df, gain_col="gain_pct")
        assert metrics.num_winners == 0
        assert metrics.avg_winner is None
```

### Widget Tests (tests/widget/test_calculation_status.py)

```python
import pytest
from PyQt6.QtCore import Qt
from src.ui.components.calculation_status import CalculationStatusIndicator
from src.ui.constants import Colors, FontSizes


class TestCalculationStatusIndicator:
    """Widget tests for CalculationStatusIndicator component."""

    def test_shows_calculating_on_start_signal(self, qtbot):  # 4.1-WGT-001
        """Indicator shows 'Calculating...' when started signal emitted."""
        indicator = CalculationStatusIndicator()
        qtbot.addWidget(indicator)
        indicator.on_calculation_started()
        assert "Calculating" in indicator.text()
        assert indicator.isVisible()

    def test_shows_ready_on_completion_signal(self, qtbot):  # 4.1-WGT-002
        """Indicator shows 'Ready' after completion signal."""
        indicator = CalculationStatusIndicator()
        qtbot.addWidget(indicator)
        indicator.on_calculation_started()
        indicator.on_calculation_completed()
        assert "Ready" in indicator.text() or not indicator.isVisible()

    def test_styled_with_theme_constants(self, qtbot):  # 4.1-WGT-003
        """Indicator uses theme constants for styling."""
        indicator = CalculationStatusIndicator()
        qtbot.addWidget(indicator)
        style = indicator.styleSheet()
        # Verify theme colors are used
        assert Colors.SIGNAL_AMBER in style or Colors.SIGNAL_CYAN in style

    def test_animation_during_calculation(self, qtbot):  # 4.1-WGT-004
        """Indicator has animation when calculating."""
        indicator = CalculationStatusIndicator()
        qtbot.addWidget(indicator)
        indicator.on_calculation_started()
        # Animation would be visible; verify property or timer active
        assert hasattr(indicator, "_animation") or indicator._is_calculating

    def test_placement_in_pnl_stats_tab(self, qtbot, app_state):  # 4.1-WGT-005
        """Indicator is placed in PnLStatsTab."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        assert hasattr(tab, "_calculation_status")
        assert isinstance(tab._calculation_status, CalculationStatusIndicator)

    def test_exported_from_components_init(self):  # 4.1-WGT-006
        """CalculationStatusIndicator exported from components."""
        from src.ui.components import CalculationStatusIndicator
        assert CalculationStatusIndicator is not None
```

### Integration Tests (tests/integration/test_filter_workflow.py)

```python
import pytest
from PyQt6.QtCore import QTimer


class TestFilteredMetricsWorkflow:
    """Integration tests for filter → metrics calculation flow."""

    def test_filter_triggers_filtered_data_updated(self, qtbot, app_state, sample_trades):
        # 4.1-INT-001
        """Applying filter triggers filtered_data_updated signal."""
        app_state.set_data(sample_trades, column_mapping)
        with qtbot.waitSignal(app_state.filtered_data_updated, timeout=1000):
            app_state.filtered_df = sample_trades.head(10)

    def test_filtered_data_updated_triggers_calculation(self, qtbot, app_state, sample_trades):
        # 4.1-INT-002
        """filtered_data_updated signal triggers metrics recalculation."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        with qtbot.waitSignal(app_state.filtered_calculation_completed, timeout=2000):
            app_state.filtered_df = sample_trades.head(10)

        assert app_state.filtered_metrics is not None

    def test_tab_navigation_recalculates_when_hash_differs(self, qtbot, app_state, sample_trades):
        # 4.1-INT-003
        """Tab navigation triggers recalculation when filtered data hash changed."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)
        app_state.filtered_df = sample_trades.head(5)

        # Simulate tab switch away and back
        tab._on_tab_hidden()
        app_state.filtered_df = sample_trades.head(10)  # Change data

        with qtbot.waitSignal(app_state.metrics_updated, timeout=2000):
            tab._on_tab_shown()

    def test_tab_navigation_skips_when_hash_unchanged(self, qtbot, app_state, sample_trades):
        # 4.1-INT-004
        """Tab navigation skips recalculation when hash unchanged."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)
        app_state.filtered_df = sample_trades.head(5)

        # Wait for initial calculation
        qtbot.waitUntil(lambda: app_state.filtered_metrics is not None, timeout=2000)
        old_metrics = app_state.filtered_metrics

        # Tab switch without data change
        tab._on_tab_hidden()
        tab._on_tab_shown()

        # Metrics should be unchanged (no recalculation)
        assert app_state.filtered_metrics is old_metrics

    def test_metrics_calculator_reused(self, qtbot, app_state, sample_trades):
        # 4.1-INT-005
        """Filtered calculation uses MetricsCalculator.calculate()."""
        from src.tabs.pnl_stats import PnLStatsTab
        from unittest.mock import patch

        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        with patch.object(tab._metrics_calculator, 'calculate', wraps=tab._metrics_calculator.calculate) as mock:
            app_state.filtered_df = sample_trades.head(10)
            qtbot.waitUntil(lambda: mock.called, timeout=2000)
            assert mock.called

    def test_filtered_metrics_stored_in_app_state(self, qtbot, app_state, sample_trades):
        # 4.1-INT-006
        """Filtered metrics stored in AppState after calculation."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        app_state.filtered_df = sample_trades.head(10)
        qtbot.waitUntil(lambda: app_state.filtered_metrics is not None, timeout=2000)

        assert app_state.filtered_metrics.num_trades == 10

    def test_filtered_flat_stake_equity_stored(self, qtbot, app_state, sample_trades):
        # 4.1-INT-007
        """Filtered flat stake equity curve stored in AppState."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        app_state.filtered_df = sample_trades.head(10)
        # Wait for debounced equity calculation
        qtbot.waitUntil(
            lambda: app_state.filtered_flat_stake_equity_curve is not None,
            timeout=3000
        )

        assert app_state.filtered_flat_stake_equity_curve is not None

    def test_filtered_kelly_equity_stored(self, qtbot, app_state, sample_trades):
        # 4.1-INT-008
        """Filtered Kelly equity curve stored in AppState."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        app_state.filtered_df = sample_trades.head(10)
        qtbot.waitUntil(
            lambda: app_state.filtered_kelly_equity_curve is not None,
            timeout=3000
        )

        assert app_state.filtered_kelly_equity_curve is not None

    def test_metrics_updated_emits_both(self, qtbot, app_state, sample_trades):
        # 4.1-INT-009
        """metrics_updated signal emits both baseline and filtered metrics."""
        from src.tabs.pnl_stats import PnLStatsTab
        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        received = []
        app_state.metrics_updated.connect(lambda b, f: received.append((b, f)))

        app_state.filtered_df = sample_trades.head(10)
        qtbot.waitUntil(lambda: len(received) > 0, timeout=2000)

        baseline, filtered = received[-1]
        assert baseline is not None
        assert filtered is not None
```

### Performance Tests (tests/integration/test_filter_workflow.py)

```python
import pytest
import time
import pandas as pd
import numpy as np


@pytest.mark.slow
class TestFilteredMetricsPerformance:
    """Performance tests for filtered metrics calculation."""

    def test_core_stats_under_100ms_10k_rows(self, qtbot, app_state):
        # 4.1-PERF-001
        """Core stats calculation completes in < 100ms with 10k rows."""
        from src.tabs.pnl_stats import PnLStatsTab

        # Generate 10k row dataset
        df = pd.DataFrame({
            "gain_pct": np.random.normal(0, 5, 10000),
            "date": pd.date_range("2020-01-01", periods=10000),
        })

        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(df, column_mapping)

        start = time.perf_counter()
        app_state.filtered_df = df
        # Wait for core stats (not equity curves)
        qtbot.waitUntil(lambda: app_state.filtered_metrics is not None, timeout=500)
        elapsed = (time.perf_counter() - start) * 1000

        assert elapsed < 100, f"Core stats took {elapsed:.1f}ms (target: <100ms)"

    def test_core_stats_under_100ms_100k_rows(self, qtbot, app_state):
        # 4.1-PERF-002
        """Core stats calculation completes in < 100ms with 100k rows."""
        from src.tabs.pnl_stats import PnLStatsTab

        df = pd.DataFrame({
            "gain_pct": np.random.normal(0, 5, 100000),
            "date": pd.date_range("2020-01-01", periods=100000),
        })

        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(df, column_mapping)

        start = time.perf_counter()
        app_state.filtered_df = df
        qtbot.waitUntil(lambda: app_state.filtered_metrics is not None, timeout=500)
        elapsed = (time.perf_counter() - start) * 1000

        assert elapsed < 100, f"Core stats took {elapsed:.1f}ms (target: <100ms)"

    def test_equity_curves_debounced_300ms(self, qtbot, app_state, sample_trades):
        # 4.1-PERF-003
        """Equity curve calculation is debounced at 300ms."""
        from src.tabs.pnl_stats import PnLStatsTab
        from src.ui.constants import Animation

        tab = PnLStatsTab(app_state)
        qtbot.addWidget(tab)
        app_state.set_data(sample_trades, column_mapping)

        # First trigger
        app_state.filtered_df = sample_trades.head(5)
        time.sleep(0.1)  # 100ms - equity not calculated yet

        # Second trigger (within debounce window)
        app_state.filtered_df = sample_trades.head(10)

        # Equity should not be calculated until 300ms after LAST trigger
        assert app_state.filtered_flat_stake_equity_curve is None

        # Wait for debounce + calculation
        qtbot.waitUntil(
            lambda: app_state.filtered_flat_stake_equity_curve is not None,
            timeout=1000
        )

        # Verify debounce constant is 300ms
        assert Animation.DEBOUNCE_METRICS == 300
```

## Recommended Execution Order

1. **P0 Unit tests** (fail fast on edge cases)
   - 4.1-UNIT-002, 4.1-UNIT-003

2. **P0 Widget tests** (status indicator core)
   - 4.1-WGT-001, 4.1-WGT-002

3. **P0 Integration tests** (core flow)
   - 4.1-INT-001, 4.1-INT-002, 4.1-INT-005, 4.1-INT-006

4. **P0 Performance tests** (requirements validation)
   - 4.1-PERF-001, 4.1-PERF-003

5. **P1 tests** (secondary functionality)
   - 4.1-UNIT-001, 4.1-UNIT-004
   - 4.1-WGT-003
   - 4.1-INT-003, 4.1-INT-004, 4.1-INT-007, 4.1-INT-008, 4.1-INT-009
   - 4.1-PERF-002

6. **P2 tests** (polish and edge cases)
   - 4.1-UNIT-005, 4.1-UNIT-006
   - 4.1-WGT-004, 4.1-WGT-005, 4.1-WGT-006

## Quality Checklist

- [x] Every AC has test coverage
- [x] Test levels are appropriate (not over-testing)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk
- [x] Test IDs follow naming convention
- [x] Scenarios are atomic and independent

## Manual Testing Requirements (Task 10)

In addition to automated tests:

1. Launch app, load sample data
2. Apply filter in Feature Explorer
3. Switch to PnL Stats tab - verify calculation triggers
4. Verify status indicator shows during calculation
5. Verify filtered metrics display alongside baseline
6. Test rapid filter changes - verify debounce behavior
7. Test empty filter result - verify graceful handling

---

## Gate YAML Block

```yaml
test_design:
  scenarios_total: 24
  by_level:
    unit: 6
    widget: 6
    integration: 9
    performance: 3
    e2e: 0
  by_priority:
    p0: 8
    p1: 10
    p2: 6
  coverage_gaps: []
```

---

Test design matrix: docs/qa/assessments/4.1-test-design-20260111.md
P0 tests identified: 8
