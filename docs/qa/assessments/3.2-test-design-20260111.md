# Test Design: Story 3.2

**Story:** Core Statistics & Distribution Data (Metrics 1-12, 24-25 prep)  
**Date:** 2026-01-11  
**Designer:** Quinn (Test Architect)

## Test Strategy Overview

- **Total test scenarios:** 32
- **Unit tests:** 18 (56%)
- **Integration tests:** 8 (25%)
- **E2E tests:** 0 (0%)
- **Widget tests:** 6 (19%)
- **Priority distribution:** P0: 8, P1: 16, P2: 8

## Test Pyramid Analysis

This story is primarily calculation and UI display work with no critical user journeys or security concerns. The test strategy emphasizes:

1. **Unit tests (56%)** - Pure calculation logic, formula correctness, edge cases
2. **Widget tests (19%)** - UI component behavior, styling, signal handling
3. **Integration tests (25%)** - Signal flow, recalculation triggers, state management
4. **E2E tests (0%)** - Not required; no critical user journeys introduced

---

## Test Scenarios by Acceptance Criteria

### AC1: Calculate metrics 1-12

**Scope:** Trades, Win Rate, Avg Winner/Loser, R:R, EV, Edge, Kelly, Fractional Kelly, EG, Median Winner/Loser

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-UNIT-001 | Unit | P0 | Edge = EV × num_trades | Core formula correctness |
| 3.2-UNIT-002 | Unit | P0 | Fractional Kelly = Kelly × (fraction/100) | Core formula correctness |
| 3.2-UNIT-003 | Unit | P0 | Expected Growth formula | Complex formula with variance |
| 3.2-UNIT-004 | Unit | P1 | Median Winner calculation | Pure statistics function |
| 3.2-UNIT-005 | Unit | P1 | Median Loser calculation | Pure statistics function |
| 3.2-UNIT-006 | Unit | P1 | Edge with zero EV | Edge case handling |
| 3.2-UNIT-007 | Unit | P1 | Fractional Kelly with negative Kelly | Edge case handling |
| 3.2-UNIT-008 | Unit | P1 | EG with zero variance | Edge case handling |
| 3.2-UNIT-009 | Unit | P2 | Metrics 1-7 unchanged (regression) | Ensure no regression |

**Test File:** `tests/unit/test_metrics.py`

```python
def test_edge_equals_ev_times_trades(sample_trades):
    """Edge = EV * num_trades (3.2-UNIT-001)."""
    calc = MetricsCalculator()
    metrics = calc.calculate(sample_trades, "gain_pct")
    expected_edge = metrics.ev * metrics.num_trades
    assert abs(metrics.edge - expected_edge) < 0.001

def test_fractional_kelly_applies_fraction(sample_trades):
    """Fractional Kelly = Kelly * (fraction/100) (3.2-UNIT-002)."""
    calc = MetricsCalculator()
    metrics = calc.calculate(sample_trades, "gain_pct", fractional_kelly_pct=50.0)
    expected = metrics.kelly * 0.5
    assert abs(metrics.fractional_kelly - expected) < 0.001

def test_expected_growth_formula(sample_trades):
    """EG = Kelly * EV - (Kelly² * variance / 2) (3.2-UNIT-003)."""
    calc = MetricsCalculator()
    metrics = calc.calculate(sample_trades, "gain_pct")
    # Calculate expected EG manually
    kelly_frac = metrics.kelly / 100  # Convert percentage to fraction
    variance = ...  # Calculate from winner/loser distributions
    expected_eg = kelly_frac * metrics.ev - (kelly_frac**2 * variance / 2)
    assert abs(metrics.expected_growth - expected_eg) < 0.001
```

---

### AC2: Prepare distribution data

**Scope:** Winner/loser arrays with min, max, mean, median, std

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-UNIT-010 | Unit | P1 | winner_min is minimum of winner_gains | Distribution statistics |
| 3.2-UNIT-011 | Unit | P1 | winner_max is maximum of winner_gains | Distribution statistics |
| 3.2-UNIT-012 | Unit | P1 | loser_min is most negative loser | Distribution statistics |
| 3.2-UNIT-013 | Unit | P1 | loser_max is least negative loser | Distribution statistics |
| 3.2-UNIT-014 | Unit | P2 | Distribution stats with single trade | Edge case |
| 3.2-UNIT-015 | Unit | P2 | Distribution stats with empty array | Edge case |

**Test File:** `tests/unit/test_metrics.py`

```python
def test_winner_min_max(sample_trades):
    """Winner min/max correctly computed (3.2-UNIT-010/011)."""
    calc = MetricsCalculator()
    metrics = calc.calculate(sample_trades, "gain_pct")
    winners = [g for g in sample_trades["gain_pct"] if g > 0]
    assert metrics.winner_min == min(winners)
    assert metrics.winner_max == max(winners)

def test_loser_min_max(sample_trades):
    """Loser min is most negative, max is least negative (3.2-UNIT-012/013)."""
    calc = MetricsCalculator()
    metrics = calc.calculate(sample_trades, "gain_pct")
    losers = [g for g in sample_trades["gain_pct"] if g <= 0]
    assert metrics.loser_min == min(losers)  # Most negative
    assert metrics.loser_max == max(losers)  # Least negative
```

---

### AC3: Display in styled metric cards with tooltips

**Scope:** MetricsGrid component, 12 cards, Observatory theme

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-WIDGET-001 | Widget | P1 | MetricsGrid displays 12 cards | UI component structure |
| 3.2-WIDGET-002 | Widget | P1 | All cards have tooltips | Tooltip presence |
| 3.2-WIDGET-003 | Widget | P2 | Cards use STANDARD variant | Styling consistency |
| 3.2-INT-001 | Integration | P1 | MetricsGrid updates on new metrics | Data flow |

**Test File:** `tests/widget/test_metrics_grid.py`

```python
def test_metrics_grid_has_12_cards(qtbot):
    """Grid displays all 12 core metrics (3.2-WIDGET-001)."""
    grid = MetricsGrid()
    qtbot.addWidget(grid)
    assert len(grid._cards) == 12

def test_all_cards_have_tooltips(qtbot):
    """Every metric card has a non-empty tooltip (3.2-WIDGET-002)."""
    grid = MetricsGrid()
    qtbot.addWidget(grid)
    for key, card in grid._cards.items():
        assert card.toolTip(), f"Card {key} missing tooltip"

def test_metrics_grid_update(qtbot):
    """Grid updates all cards when update_metrics called (3.2-INT-001)."""
    grid = MetricsGrid()
    qtbot.addWidget(grid)
    metrics = TradingMetrics(num_trades=100, win_rate=60.0, ...)
    grid.update_metrics(metrics)
    # Verify values updated
    assert "100" in grid._cards["num_trades"]._value_widget.text()
```

---

### AC4: Color coding for positive/negative values

**Scope:** SIGNAL_CYAN for positive, SIGNAL_CORAL for negative

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-WIDGET-004 | Widget | P1 | Positive values show cyan | Color coding |
| 3.2-WIDGET-005 | Widget | P1 | Negative values show coral | Color coding |
| 3.2-WIDGET-006 | Widget | P2 | Zero/None values show primary | Color coding |

**Test File:** `tests/widget/test_metrics_grid.py`

```python
def test_positive_value_shows_cyan(qtbot):
    """Positive values display in SIGNAL_CYAN (3.2-WIDGET-004)."""
    grid = MetricsGrid()
    qtbot.addWidget(grid)
    metrics = TradingMetrics(num_trades=100, win_rate=60.0, avg_winner=5.0, ...)
    grid.update_metrics(metrics)
    style = grid._cards["win_rate"]._value_widget.styleSheet()
    assert Colors.SIGNAL_CYAN.lower() in style.lower()

def test_negative_value_shows_coral(qtbot):
    """Negative values display in SIGNAL_CORAL (3.2-WIDGET-005)."""
    grid = MetricsGrid()
    qtbot.addWidget(grid)
    metrics = TradingMetrics(num_trades=100, avg_loser=-3.0, ...)
    grid.update_metrics(metrics)
    style = grid._cards["avg_loser"]._value_widget.styleSheet()
    assert Colors.SIGNAL_CORAL.lower() in style.lower()
```

---

### AC5: Recalculate on user input changes

**Scope:** Signal connections, debouncing, state synchronization

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-INT-002 | Integration | P0 | Metrics recalculate on fractional_kelly change | Core functionality |
| 3.2-INT-003 | Integration | P0 | Metrics recalculate on stop_loss change | Core functionality |
| 3.2-INT-004 | Integration | P0 | Metrics recalculate on efficiency change | Core functionality |
| 3.2-INT-005 | Integration | P1 | Recalculation is debounced (300ms) | Performance |
| 3.2-INT-006 | Integration | P1 | Empty state shown when no data | State handling |
| 3.2-INT-007 | Integration | P2 | Rapid input changes don't cause errors | Stability |

**Test File:** `tests/widget/test_pnl_stats.py`

```python
def test_metrics_recalculate_on_fractional_kelly_change(qtbot, app_state):
    """Metrics update when fractional kelly changes (3.2-INT-002)."""
    tab = PnLStatsTab(app_state)
    qtbot.addWidget(tab)
    # Load data
    app_state.set_baseline_df(sample_df)
    initial_frac_kelly = tab._metrics_grid._cards["fractional_kelly"].value()
    # Change input
    app_state.metrics_user_inputs_changed.emit(
        MetricsUserInputs(fractional_kelly=50.0)
    )
    qtbot.wait(350)  # Wait for debounce
    new_frac_kelly = tab._metrics_grid._cards["fractional_kelly"].value()
    assert new_frac_kelly != initial_frac_kelly

def test_recalculation_debounced(qtbot, app_state):
    """Multiple rapid changes only trigger one recalculation (3.2-INT-005)."""
    tab = PnLStatsTab(app_state)
    qtbot.addWidget(tab)
    calc_count = 0
    original_calc = tab._recalculate_metrics
    def counting_calc():
        nonlocal calc_count
        calc_count += 1
        original_calc()
    tab._recalculate_metrics = counting_calc
    # Emit 5 rapid changes
    for i in range(5):
        app_state.metrics_user_inputs_changed.emit(
            MetricsUserInputs(fractional_kelly=20.0 + i)
        )
    qtbot.wait(350)  # Wait for debounce
    assert calc_count == 1, "Should only recalculate once due to debounce"
```

---

### AC6: Calculation < 100ms for 100k rows

**Scope:** Performance requirement

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-UNIT-016 | Unit | P0 | Calculation < 100ms for 100k rows | Performance requirement |
| 3.2-UNIT-017 | Unit | P1 | Calculation < 50ms for 10k rows | Sanity check |

**Test File:** `tests/unit/test_metrics.py`

```python
@pytest.mark.slow
def test_metrics_calculation_under_100ms(large_dataset_100k):
    """Calculation completes in < 100ms for 100k rows (3.2-UNIT-016)."""
    calc = MetricsCalculator()
    start = time.perf_counter()
    metrics = calc.calculate(large_dataset_100k, "gain_pct", fractional_kelly_pct=25.0)
    elapsed_ms = (time.perf_counter() - start) * 1000
    assert elapsed_ms < 100, f"Calculation took {elapsed_ms:.1f}ms, expected < 100ms"
    assert metrics.num_trades == 100000

@pytest.fixture
def large_dataset_100k():
    """Generate 100k row dataset with realistic distribution."""
    np.random.seed(42)
    n = 100000
    gains = np.concatenate([
        np.random.normal(5.0, 3.0, int(n * 0.55)),   # Winners (~55%)
        np.random.normal(-3.0, 2.0, int(n * 0.45))   # Losers (~45%)
    ])
    np.random.shuffle(gains)
    return pd.DataFrame({"gain_pct": gains})
```

---

### Model Tests (TradingMetrics extensions)

| ID | Level | Priority | Test | Justification |
|----|-------|----------|------|---------------|
| 3.2-UNIT-018 | Unit | P1 | TradingMetrics has all new fields | Model completeness |
| 3.2-INT-008 | Integration | P2 | TradingMetrics.empty() includes new fields | Factory method |

**Test File:** `tests/unit/test_models.py`

```python
def test_trading_metrics_has_new_fields():
    """TradingMetrics includes all Story 3.2 fields (3.2-UNIT-018)."""
    metrics = TradingMetrics(
        num_trades=100, win_rate=60.0, avg_winner=5.0, avg_loser=-3.0,
        rr_ratio=1.67, ev=1.8, kelly=20.0,
        edge=180.0, fractional_kelly=5.0, expected_growth=0.5,
        median_winner=4.5, median_loser=-2.8,
        winner_min=0.5, winner_max=15.0, loser_min=-8.0, loser_max=-0.1
    )
    assert metrics.edge == 180.0
    assert metrics.fractional_kelly == 5.0
    assert metrics.expected_growth == 0.5
    assert metrics.median_winner == 4.5
    assert metrics.median_loser == -2.8
    assert metrics.winner_min == 0.5
    assert metrics.winner_max == 15.0
    assert metrics.loser_min == -8.0
    assert metrics.loser_max == -0.1

def test_trading_metrics_empty_includes_new_fields():
    """TradingMetrics.empty() initializes new fields to None (3.2-INT-008)."""
    metrics = TradingMetrics.empty()
    assert metrics.edge is None
    assert metrics.fractional_kelly is None
    assert metrics.expected_growth is None
    assert metrics.median_winner is None
    assert metrics.median_loser is None
    assert metrics.winner_min is None
    assert metrics.winner_max is None
    assert metrics.loser_min is None
    assert metrics.loser_max is None
```

---

## Risk Coverage Mapping

| Risk ID | Test IDs | Coverage |
|---------|----------|----------|
| PERF-001 | 3.2-UNIT-016, 3.2-UNIT-017 | Direct |
| TECH-001 | 3.2-UNIT-001, 3.2-UNIT-002, 3.2-UNIT-003 | Direct |
| TECH-002 | 3.2-INT-002, 3.2-INT-003, 3.2-INT-007 | Direct |
| DATA-001 | 3.2-UNIT-006, 3.2-UNIT-007, 3.2-UNIT-008 | Direct |
| TECH-003 | 3.2-INT-005 | Direct |
| DATA-002 | 3.2-UNIT-014, 3.2-UNIT-015 | Direct |
| TECH-004 | 3.2-WIDGET-004, 3.2-WIDGET-005, 3.2-WIDGET-006 | Direct |
| OPS-001 | 3.2-WIDGET-002 | Direct |

---

## Test File Summary

| File | Tests | Priority Mix |
|------|-------|--------------|
| `tests/unit/test_metrics.py` | 17 | 4×P0, 9×P1, 4×P2 |
| `tests/unit/test_models.py` | 1 | 1×P1 |
| `tests/widget/test_metrics_grid.py` | 6 | 4×P1, 2×P2 |
| `tests/widget/test_pnl_stats.py` | 7 | 4×P0, 2×P1, 1×P2 |

---

## Recommended Execution Order

1. **P0 Unit tests** (fail fast on calculation errors)
   - 3.2-UNIT-001, 002, 003, 016
2. **P0 Integration tests** (verify core signal flow)
   - 3.2-INT-002, 003, 004
3. **P1 Unit tests** (formula edge cases)
   - 3.2-UNIT-004 through 015, 017, 018
4. **P1 Widget tests** (UI component behavior)
   - 3.2-WIDGET-001, 002, 004, 005
5. **P1 Integration tests** (state management)
   - 3.2-INT-005, 006
6. **P2 tests** (nice-to-have coverage)
   - Remaining tests as time permits

---

## Fixtures Required

```python
# conftest.py additions

@pytest.fixture
def sample_trades():
    """50-trade sample with known distribution."""
    return pd.DataFrame({
        "gain_pct": [5.0, -3.0, 8.0, -2.0, 3.0] * 10  # 50 trades
    })

@pytest.fixture
def large_dataset_100k():
    """100k row dataset for performance testing."""
    np.random.seed(42)
    n = 100000
    gains = np.concatenate([
        np.random.normal(5.0, 3.0, int(n * 0.55)),
        np.random.normal(-3.0, 2.0, int(n * 0.45))
    ])
    np.random.shuffle(gains)
    return pd.DataFrame({"gain_pct": gains})

@pytest.fixture
def app_state():
    """Fresh AppState instance for integration tests."""
    return AppState()
```

---

## Quality Checklist

- [x] Every AC has test coverage
- [x] Test levels are appropriate (not over-testing)
- [x] No duplicate coverage across levels
- [x] Priorities align with business risk
- [x] Test IDs follow naming convention
- [x] Scenarios are atomic and independent
- [x] All identified risks have test coverage

---

## Gate YAML Block

```yaml
test_design:
  scenarios_total: 32
  by_level:
    unit: 18
    integration: 8
    widget: 6
    e2e: 0
  by_priority:
    p0: 8
    p1: 16
    p2: 8
  coverage_gaps: []
```

---

**Test design matrix:** docs/qa/assessments/3.2-test-design-20260111.md  
**P0 tests identified:** 8
