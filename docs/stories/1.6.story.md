# Story 1.6: Core Metrics Calculation & Display

## Status

Done

## Story

**As a** user,
**I want** to see key trading metrics for my baseline data,
**so that** I can assess my overall trading performance.

## Acceptance Criteria

1. Calculate 7 core metrics: Number of Trades, Win Rate, Avg Winner, Avg Loser, R:R Ratio, EV, Kelly
2. Win Rate uses mapped Win/Loss column or derives from Gain % per configuration
3. Prepare distribution data (winner/loser arrays with statistics)
4. Display metrics in styled cards with appropriate formatting
5. Color coding: positive (plasma-cyan), negative (solar-coral)
6. Edge case handling (no winners, no losers, empty dataset)
7. Calculation completes in < 100ms for 100k rows
8. User inputs panel with Stop Loss % (default 8%) and Efficiency % (default 5%)
9. Calculate stop_adjusted_gain_pct: if mae_pct > stop_loss then -stop_loss, else gain_pct
10. Calculate efficiency_adjusted_gain_pct = stop_adjusted_gain_pct - efficiency
11. All metrics calculated using efficiency_adjusted_gain_pct
12. User input changes trigger metric recalculation with 300ms debounce

## Tasks / Subtasks

- [x] Task 1: Create TradingMetrics Dataclass (AC: 1, 3)
  - [x] Create `TradingMetrics` dataclass in `src/core/models.py`
  - [x] Add 7 core metric fields: `num_trades`, `win_rate`, `avg_winner`, `avg_loser`, `rr_ratio`, `ev`, `kelly`
  - [x] Add distribution data fields: `winner_count`, `loser_count`, `winner_std`, `loser_std`, `winner_gains`, `loser_gains`
  - [x] Add `@classmethod empty()` returning TradingMetrics with all None/zero values
  - [x] Add docstrings and type hints

- [x] Task 2: Create MetricsCalculator Class (AC: 1, 2, 3, 6, 7)
  - [x] Create `src/core/metrics.py` with `MetricsCalculator` class
  - [x] Implement `calculate(df, gain_col, win_loss_col, derived, breakeven_is_win) -> TradingMetrics`:
    - **Number of Trades**: `len(df)`
    - **Win/Loss Classification**: If `win_loss_col` provided, use it; else derive from `gain_col`:
      - Winner: `gain_pct > 0` (or `>= 0` if breakeven_is_win)
      - Loser: `gain_pct < 0` (or `<= 0` if breakeven_is_win)
    - **Win Rate**: `(winners / total) * 100`
    - **Avg Winner**: Mean of winner gains (None if no winners)
    - **Avg Loser**: Mean of loser gains (None if no losers) - should be negative
    - **R:R Ratio**: `abs(avg_winner / avg_loser)` (None if either is None)
    - **EV**: `(win_rate/100 * avg_winner) + ((1 - win_rate/100) * avg_loser)` (None if incomplete)
    - **Kelly**: `win_rate/100 - ((1 - win_rate/100) / rr_ratio)` (None if rr_ratio is None)
  - [x] Populate distribution data: `winner_gains`, `loser_gains`, `winner_std`, `loser_std`
  - [x] Handle edge cases:
    - Empty DataFrame: return `TradingMetrics.empty()`
    - No winners: `win_rate=0`, `avg_winner=None`, `rr_ratio=None`, `kelly=None`
    - No losers: `win_rate=100`, `avg_loser=None`, `rr_ratio=None`, `kelly=None`
  - [x] Use vectorized pandas operations for < 100ms on 100k rows
  - [x] Add type hints and docstrings

- [x] Task 3: Create MetricCard Widget (AC: 4, 5)
  - [x] Create `src/ui/components/metric_card.py` with `MetricCard(QFrame)` class
  - [x] Accept constructor params: `label: str`, `variant: str = "standard"`
  - [x] Implement `update_value(value, format_spec=".2f")` method
  - [x] Display label at top (TEXT_SECONDARY, Fonts.UI, 12px)
  - [x] Display value below (TEXT_PRIMARY, Fonts.DATA, 24px for standard)
  - [x] Style with BG_ELEVATED background, rounded corners (8px)
  - [x] Add padding using Spacing.MD (12px)
  - [x] Color-code values based on sign:
    - Positive: SIGNAL_CYAN (#00FFD4)
    - Negative: SIGNAL_CORAL (#FF4757)
    - Zero/None: TEXT_PRIMARY (#F4F4F8)
  - [x] Update `src/ui/components/__init__.py` to export `MetricCard`

- [x] Task 4: Create MetricsPanel Widget (AC: 4, 5)
  - [x] Create `MetricsPanel` widget in `src/tabs/data_input.py`
  - [x] Display 7 MetricCards in a responsive grid layout (3 columns on wide, 2 on narrow)
  - [x] Card labels: "Trades", "Win Rate", "Avg Winner", "Avg Loser", "R:R Ratio", "EV", "Kelly"
  - [x] Format specifications:
    - Trades: `"{:,}"` (integer with thousands separator)
    - Win Rate: `"{:.1f}%"` (one decimal + percent)
    - Avg Winner/Loser: `"{:+.2f}%"` (sign + two decimals + percent)
    - R:R Ratio: `"{:.2f}:1"` (two decimals + :1)
    - EV: `"{:+.2f}%"` (sign + two decimals + percent)
    - Kelly: `"{:.1f}%"` (one decimal + percent)
  - [x] Show "—" for None values
  - [x] Add title: "Baseline Metrics" with Fonts.UI, H2 size (18px)

- [x] Task 5: Integrate Metrics into Data Flow (AC: 1, 2, 4)
  - [x] In `DataInputTab`, after first trigger calculation:
    - Get `baseline_df` and `column_mapping` from AppState
    - Calculate metrics via `MetricsCalculator.calculate()`
    - Store in `app_state.baseline_metrics`
    - Emit `baseline_calculated` signal with TradingMetrics
  - [x] Display `MetricsPanel` below `BaselineInfoCard`
  - [x] Connect MetricsPanel to `baseline_calculated` signal to update display
  - [x] Log at INFO level: "Calculated baseline metrics: {n} trades, {win_rate}% win rate"

- [x] Task 6: Update AppState Type Hints (AC: 1)
  - [x] Update `app_state.py` to import `TradingMetrics` from models
  - [x] Change `baseline_metrics` and `filtered_metrics` type hints from `object` to `TradingMetrics | None`
  - [x] Update docstrings to reference TradingMetrics

- [x] Task 7: Write Unit Tests for TradingMetrics (AC: 1, 3, 6)
  - [x] Create `tests/unit/test_metrics.py`
  - [x] Test `TradingMetrics.empty()` returns all None/zero values
  - [x] Test dataclass fields are correctly typed
  - [x] Test distribution data fields (winner_gains, loser_gains lists)

- [x] Task 8: Write Unit Tests for MetricsCalculator (AC: 1, 2, 6, 7)
  - [x] Test basic calculation with balanced wins/losses
  - [x] Test win rate calculation is correct percentage
  - [x] Test avg winner/loser calculations
  - [x] Test R:R ratio calculation
  - [x] Test EV calculation: `(0.6 * 2.0) + (0.4 * -1.0) = 0.8`
  - [x] Test Kelly calculation: `0.6 - (0.4 / 2.0) = 0.4` (40%)
  - [x] Test empty DataFrame returns `TradingMetrics.empty()`
  - [x] Test no winners edge case (all losses)
  - [x] Test no losers edge case (all wins)
  - [x] Test derived win/loss with breakeven_is_win=True vs False
  - [x] Test performance: < 100ms for 100k rows (mark with `@pytest.mark.slow`)

- [x] Task 9: Write Widget Tests for MetricCard (AC: 4, 5)
  - [x] Create `tests/widget/test_metric_card.py`
  - [x] Test card displays label and value correctly
  - [x] Test positive values use SIGNAL_CYAN color
  - [x] Test negative values use SIGNAL_CORAL color
  - [x] Test None values display "—"
  - [x] Test format_spec is applied correctly

- [x] Task 10: Write Widget Tests for MetricsPanel (AC: 4)
  - [x] Create `tests/widget/test_metrics_panel.py`
  - [x] Test panel displays all 7 metric cards
  - [x] Test update with TradingMetrics updates all cards
  - [x] Test formatting is correct for each metric type

- [x] Task 11: Verify Integration (AC: 1-7)
  - [x] Run `uv run python -m src.main` and verify:
    - [x] After baseline calculation, metrics panel appears
    - [x] All 7 metrics display with correct values
    - [x] Positive values in cyan, negative in coral
    - [x] Edge cases handled (test with all-win or all-loss data)
  - [x] Run `make lint` and fix any issues
  - [x] Run `make typecheck` and fix any issues
  - [x] Run `make test` and verify all tests pass
  - [x] Run performance test for 100k rows

### RETROFIT TASKS (Added 2026-01-11)

- [x] Task 12: Create AdjustmentParams Dataclass (AC: 8, 9, 10)
  - [x] Add `AdjustmentParams` dataclass to `src/core/models.py`
  - [x] Fields: `stop_loss: float = 8.0`, `efficiency: float = 5.0`
  - [x] Method: `calculate_adjusted_gain(gain_pct: float, mae_pct: float) -> float`
  - [x] Method: `calculate_adjusted_gains(df, gain_col, mae_col) -> pd.Series` (vectorized)
  - [x] Add unit tests for adjustment calculations

- [x] Task 13: Create Adjustment Inputs Panel (AC: 8, 12)
  - [x] Create `AdjustmentInputsPanel` widget in `src/tabs/data_input.py`
  - [x] Stop Loss % input with QDoubleSpinBox (0-100, default 8, step 0.5)
  - [x] Efficiency % input with QDoubleSpinBox (0-100, default 5, step 0.5)
  - [x] Style consistent with existing panels (BG_ELEVATED, rounded corners)
  - [x] Emit `params_changed` signal on value change
  - [x] Add widget tests

- [x] Task 14: Integrate Adjustments into Metrics Calculation (AC: 9, 10, 11)
  - [x] Update `MetricsCalculator.calculate()` to accept `AdjustmentParams` and `mae_col`
  - [x] Calculate adjusted gains before metric computation using vectorized operations
  - [x] Store `adjustment_params` in `AppState`
  - [x] Update tests for adjusted calculations
  - [x] Verify metrics use efficiency_adjusted_gain_pct

- [x] Task 15: Wire User Input Changes to Recalculation (AC: 12)
  - [x] Connect `AdjustmentInputsPanel.params_changed` to recalculation
  - [x] Implement 300ms debounce using QTimer
  - [x] Update `AppState.adjustment_params` on change
  - [x] Emit `adjustment_params_changed` signal
  - [x] Trigger metrics recalculation and display update
  - [x] Add integration tests

- [x] Task 16: Verify Retrofit Integration (AC: 8-12)
  - [x] Run `uv run python -m src.main` and verify:
    - [x] Adjustment inputs panel appears in Data Input tab
    - [x] Default values: Stop Loss 8%, Efficiency 5%
    - [x] Changing inputs triggers metric recalculation
    - [x] Metrics reflect adjusted gain values
  - [x] Run `make lint` and fix any issues
  - [x] Run `make typecheck` and fix any issues
  - [x] Run `make test` and verify all tests pass

## Dev Notes

### Previous Story Insights
[Source: Story 1.5 Dev Agent Record]

- AppState class exists at `src/core/app_state.py`
- AppState already has `baseline_metrics` and `filtered_metrics` properties (typed as `object | None`)
- `baseline_calculated` signal exists and emits after first trigger calculation
- `BaselineInfoCard` widget exists in `src/tabs/data_input.py`
- `ColumnMapping` dataclass exists in `src/core/models.py` with `win_loss`, `win_loss_derived`, `breakeven_is_win` fields
- First trigger integration emits `data_loaded` and `baseline_calculated` signals after mapping
- UI Constants available in `src/ui/constants.py`: `Colors`, `Fonts`, `Spacing`
- `TradingMetrics` dataclass does NOT exist yet - must be created in this story
- `MetricsCalculator` class does NOT exist yet - must be created in this story

### TradingMetrics Design
[Source: architecture/4-data-models.md#TradingMetrics]

```python
# src/core/models.py (add to existing file)
from dataclasses import dataclass, field

@dataclass
class TradingMetrics:
    """Trading metrics for baseline and filtered data analysis.
    
    Story 1.6 implements 7 core metrics. Additional metrics (8-25) 
    will be added in Epic 3.
    """
    
    # Core Statistics (Story 1.6)
    num_trades: int
    win_rate: float | None          # Percentage (0-100)
    avg_winner: float | None        # Percentage
    avg_loser: float | None         # Percentage (negative)
    rr_ratio: float | None          # Risk:Reward ratio
    ev: float | None                # Expected Value percentage
    kelly: float | None             # Kelly criterion percentage
    
    # Distribution Data (Story 1.6)
    winner_count: int | None = None
    loser_count: int | None = None
    winner_std: float | None = None
    loser_std: float | None = None
    winner_gains: list[float] = field(default_factory=list)
    loser_gains: list[float] = field(default_factory=list)
    
    @classmethod
    def empty(cls) -> "TradingMetrics":
        """Return metrics with all None/zero values for empty datasets."""
        return cls(
            num_trades=0,
            win_rate=None,
            avg_winner=None,
            avg_loser=None,
            rr_ratio=None,
            ev=None,
            kelly=None,
            winner_count=0,
            loser_count=0,
            winner_std=None,
            loser_std=None,
            winner_gains=[],
            loser_gains=[],
        )
```

### MetricsCalculator Design
[Source: architecture/5-components.md#MetricsCalculator]

```python
# src/core/metrics.py
import logging
import pandas as pd
from src.core.models import TradingMetrics

logger = logging.getLogger(__name__)

class MetricsCalculator:
    """Calculate trading metrics from DataFrame.
    
    Supports both explicit Win/Loss column and derived classification
    based on Gain % column.
    """
    
    def calculate(
        self,
        df: pd.DataFrame,
        gain_col: str,
        win_loss_col: str | None = None,
        derived: bool = False,
        breakeven_is_win: bool = False,
    ) -> TradingMetrics:
        """Calculate all 7 core trading metrics.
        
        Args:
            df: DataFrame with trade data
            gain_col: Column name for gain percentage
            win_loss_col: Optional column name for explicit win/loss
            derived: If True, derive win/loss from gain_col
            breakeven_is_win: When deriving, treat 0% as win
            
        Returns:
            TradingMetrics with all calculated values
        """
        if len(df) == 0:
            logger.debug("Empty DataFrame, returning empty metrics")
            return TradingMetrics.empty()
        
        gains = df[gain_col].astype(float)
        
        # Classify wins/losses
        if win_loss_col and not derived:
            # Use explicit column
            winners_mask = df[win_loss_col].isin(["W", "Win", "WIN", 1, True])
            losers_mask = ~winners_mask
        else:
            # Derive from gain percentage
            if breakeven_is_win:
                winners_mask = gains >= 0
                losers_mask = gains < 0
            else:
                winners_mask = gains > 0
                losers_mask = gains <= 0
        
        winner_gains = gains[winners_mask].tolist()
        loser_gains = gains[losers_mask].tolist()
        
        num_trades = len(df)
        winner_count = len(winner_gains)
        loser_count = len(loser_gains)
        
        # Win rate
        win_rate = (winner_count / num_trades) * 100 if num_trades > 0 else None
        
        # Averages
        avg_winner = sum(winner_gains) / winner_count if winner_count > 0 else None
        avg_loser = sum(loser_gains) / loser_count if loser_count > 0 else None
        
        # R:R Ratio
        rr_ratio = None
        if avg_winner is not None and avg_loser is not None and avg_loser != 0:
            rr_ratio = abs(avg_winner / avg_loser)
        
        # Expected Value
        ev = None
        if win_rate is not None and avg_winner is not None and avg_loser is not None:
            ev = (win_rate / 100 * avg_winner) + ((1 - win_rate / 100) * avg_loser)
        
        # Kelly Criterion
        kelly = None
        if win_rate is not None and rr_ratio is not None and rr_ratio > 0:
            kelly = (win_rate / 100) - ((1 - win_rate / 100) / rr_ratio)
            kelly = kelly * 100  # Convert to percentage
        
        # Standard deviations
        winner_std = float(pd.Series(winner_gains).std()) if winner_count > 1 else None
        loser_std = float(pd.Series(loser_gains).std()) if loser_count > 1 else None
        
        logger.info(
            "Calculated baseline metrics: %d trades, %.1f%% win rate",
            num_trades,
            win_rate or 0,
        )
        
        return TradingMetrics(
            num_trades=num_trades,
            win_rate=win_rate,
            avg_winner=avg_winner,
            avg_loser=avg_loser,
            rr_ratio=rr_ratio,
            ev=ev,
            kelly=kelly,
            winner_count=winner_count,
            loser_count=loser_count,
            winner_std=winner_std,
            loser_std=loser_std,
            winner_gains=winner_gains,
            loser_gains=loser_gains,
        )
```

### MetricCard Design
[Source: architecture/5-components.md#MetricCard]

```python
# src/ui/components/metric_card.py
from PyQt6.QtWidgets import QFrame, QVBoxLayout, QLabel
from PyQt6.QtCore import Qt
from src.ui.constants import Colors, Fonts, Spacing

class MetricCard(QFrame):
    """Display a single metric value with label.
    
    Variants:
        HERO: 56px value, for Comparison Ribbon (Epic 3)
        STANDARD: 24px value, for metrics grid
        COMPACT: 16px value, for dense displays
    """
    
    HERO = "hero"
    STANDARD = "standard"
    COMPACT = "compact"
    
    def __init__(
        self,
        label: str,
        variant: str = STANDARD,
        parent=None,
    ) -> None:
        super().__init__(parent)
        self.setObjectName("metricCard")
        self._label = label
        self._variant = variant
        self._value: float | None = None
        self._setup_ui()
        self._apply_style()
    
    def _setup_ui(self) -> None:
        layout = QVBoxLayout(self)
        layout.setContentsMargins(
            Spacing.MD, Spacing.MD, Spacing.MD, Spacing.MD
        )
        layout.setSpacing(Spacing.XS)
        
        # Label
        self._label_widget = QLabel(self._label)
        self._label_widget.setObjectName("label")
        layout.addWidget(self._label_widget)
        
        # Value
        self._value_widget = QLabel("—")
        self._value_widget.setObjectName("value")
        layout.addWidget(self._value_widget)
    
    def _apply_style(self) -> None:
        font_size = {
            self.HERO: 56,
            self.STANDARD: 24,
            self.COMPACT: 16,
        }.get(self._variant, 24)
        
        self.setStyleSheet(f"""
            QFrame#metricCard {{
                background-color: {Colors.BG_ELEVATED};
                border-radius: 8px;
            }}
            QLabel#label {{
                color: {Colors.TEXT_SECONDARY};
                font-family: {Fonts.UI};
                font-size: 12px;
            }}
            QLabel#value {{
                color: {Colors.TEXT_PRIMARY};
                font-family: {Fonts.DATA};
                font-size: {font_size}px;
            }}
        """)
    
    def update_value(
        self,
        value: float | int | None,
        format_spec: str = ".2f",
    ) -> None:
        """Update displayed value.
        
        Args:
            value: The value to display (None shows "—")
            format_spec: Format specification for the value
        """
        self._value = value
        
        if value is None:
            self._value_widget.setText("—")
            self._value_widget.setStyleSheet(f"""
                color: {Colors.TEXT_PRIMARY};
                font-family: {Fonts.DATA};
            """)
            return
        
        # Format value
        try:
            if isinstance(value, int):
                text = f"{value:,}"
            else:
                text = f"{value:{format_spec}}"
        except (ValueError, TypeError):
            text = str(value)
        
        self._value_widget.setText(text)
        
        # Color code based on sign
        if isinstance(value, (int, float)):
            if value > 0:
                color = Colors.SIGNAL_CYAN
            elif value < 0:
                color = Colors.SIGNAL_CORAL
            else:
                color = Colors.TEXT_PRIMARY
        else:
            color = Colors.TEXT_PRIMARY
        
        self._value_widget.setStyleSheet(f"""
            color: {color};
            font-family: {Fonts.DATA};
        """)
```

### UI Constants
[Source: src/ui/constants.py - verified from Story 1.5]

```python
class Colors:
    BG_BASE = "#0C0C12"
    BG_SURFACE = "#141420"
    BG_ELEVATED = "#1E1E2C"
    BG_BORDER = "#2A2A3A"
    SIGNAL_CYAN = "#00FFD4"   # Positive values
    SIGNAL_CORAL = "#FF4757"  # Negative values
    SIGNAL_AMBER = "#FFAA00"  # Warning
    SIGNAL_BLUE = "#4A9EFF"   # Reference
    TEXT_PRIMARY = "#F4F4F8"
    TEXT_SECONDARY = "#9898A8"
    TEXT_DISABLED = "#5C5C6C"

class Fonts:
    DATA = "Azeret Mono"
    UI = "Geist"

class Spacing:
    XS = 4
    SM = 8
    MD = 12
    LG = 16
    XL = 24
    XXL = 32
```

### Metrics Formulas Reference
[Source: PRD Story 1.6 AC 1]

| Metric | Formula | Notes |
|--------|---------|-------|
| **Number of Trades** | `len(df)` | Total rows in baseline |
| **Win Rate** | `(winners / total) × 100` | Percentage 0-100 |
| **Avg Winner** | `mean(winner_gains)` | Positive percentage |
| **Avg Loser** | `mean(loser_gains)` | Negative percentage |
| **R:R Ratio** | `abs(avg_winner / avg_loser)` | Dimensionless ratio |
| **EV** | `(win_rate × avg_winner) + (loss_rate × avg_loser)` | Expected value per trade |
| **Kelly** | `win_rate - (loss_rate / rr_ratio)` | Optimal bet fraction × 100 |

### Win/Loss Classification Logic
[Source: PRD Story 1.6 AC 2, architecture/4-data-models.md]

```python
# From ColumnMapping (Story 1.4)
if mapping.win_loss_derived:
    # Derive from gain_pct
    if mapping.breakeven_is_win:
        winners = df[gain_col] >= 0
        losers = df[gain_col] < 0
    else:
        winners = df[gain_col] > 0
        losers = df[gain_col] <= 0
else:
    # Use explicit win_loss column
    winners = df[win_loss_col].isin(["W", "Win", "WIN", 1, True])
    losers = ~winners
```

### Workflow Integration
[Source: architecture/6-core-workflows.md#workflow-1]

Story 1.6 extends Workflow 1 with metrics calculation:

```
File Load → Column Mapping → First Trigger → Metrics Calculation → Display
                                    │                  │
                                    │                  ▼
                                    │         MetricsCalculator.calculate()
                                    │                  │
                                    │                  ▼
                                    ▼         Store in AppState.baseline_metrics
                            baseline_df               │
                                                      ▼
                                              Emit baseline_calculated(metrics)
                                                      │
                                                      ▼
                                              MetricsPanel updates display
```

### File Locations
[Source: architecture/2-high-level-architecture.md#repository-structure]

| File | Purpose |
|------|---------|
| `src/core/models.py` | Add TradingMetrics dataclass (MODIFY) |
| `src/core/metrics.py` | MetricsCalculator class (NEW) |
| `src/core/app_state.py` | Update type hints for TradingMetrics (MODIFY) |
| `src/ui/components/metric_card.py` | MetricCard widget (NEW) |
| `src/ui/components/__init__.py` | Export MetricCard (MODIFY) |
| `src/tabs/data_input.py` | Add MetricsPanel, integrate calculation (MODIFY) |
| `tests/unit/test_metrics.py` | MetricsCalculator unit tests (NEW) |
| `tests/widget/test_metric_card.py` | MetricCard widget tests (NEW) |
| `tests/widget/test_metrics_panel.py` | MetricsPanel widget tests (NEW) |

### Edge Cases
[Source: PRD Story 1.6 AC 6]

| Edge Case | Handling |
|-----------|----------|
| Empty DataFrame | Return `TradingMetrics.empty()` with `num_trades=0`, all others `None` |
| No winners (all losses) | `win_rate=0`, `avg_winner=None`, `rr_ratio=None`, `ev=None`, `kelly=None` |
| No losers (all wins) | `win_rate=100`, `avg_loser=None`, `rr_ratio=None` |
| Single trade (winner) | `num_trades=1`, `win_rate=100`, `winner_std=None` (need 2+ for std) |
| Zero gain (breakeven) | Classified based on `breakeven_is_win` setting |

### Performance Requirement
[Source: PRD Story 1.6 AC 7]

Calculation must complete in < 100ms for 100k rows. Use vectorized pandas operations:

```python
# GOOD: Vectorized boolean masking
winners_mask = gains > 0
winner_gains = gains[winners_mask].tolist()

# GOOD: Vectorized aggregations
avg_winner = gains[winners_mask].mean()

# BAD: Python loop over rows
for i, row in df.iterrows():  # DON'T DO THIS
    if row['gain_pct'] > 0:
        winners.append(row['gain_pct'])
```

### Coding Standards
[Source: architecture/9-coding-standards.md]

- **Line length**: 100 characters
- **Type hints**: Required for all public APIs
- **Naming**:
  - Modules: snake_case (`metrics.py`, `metric_card.py`)
  - Classes: PascalCase (`MetricsCalculator`, `MetricCard`)
  - Functions: snake_case (`calculate()`, `update_value()`)
- **Logging**: Use `logging.getLogger(__name__)` pattern

### Logging Pattern
[Source: architecture/9-coding-standards.md#logging-guidelines]

```python
import logging
logger = logging.getLogger(__name__)

# INFO: Calculation milestones
logger.info("Calculated baseline metrics: %d trades, %.1f%% win rate", num_trades, win_rate)

# DEBUG: Internal state
logger.debug("Winner count: %d, Loser count: %d", winner_count, loser_count)
```

## Testing

### Test File Locations
[Source: architecture/8-testing-strategy.md]

- Unit tests: `tests/unit/`
- Widget tests: `tests/widget/`
- Shared fixtures: `tests/conftest.py`

### Testing Standards
[Source: architecture/8-testing-strategy.md]

- Use pytest as test framework
- Use pytest-qt for widget testing
- Add `qtbot: QtBot` type annotation to widget test methods
- Import: `from pytestqt.qtbot import QtBot`
- Mark slow tests with `@pytest.mark.slow`

### Test Cases for Story 1.6

**Unit Tests** (`tests/unit/test_metrics.py`):

```python
import pytest
import pandas as pd
from src.core.metrics import MetricsCalculator
from src.core.models import TradingMetrics

def test_metrics_basic():
    """Basic metrics calculation with balanced wins/losses."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [2.0, 3.0, -1.0, -2.0, 1.5, -0.5],  # 3 wins, 3 losses
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    
    assert metrics.num_trades == 6
    assert metrics.win_rate == 50.0
    assert metrics.winner_count == 3
    assert metrics.loser_count == 3

def test_metrics_win_rate():
    """Win rate calculation is correct percentage."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [1.0, 2.0, 3.0, -1.0],  # 75% win rate
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.win_rate == 75.0

def test_metrics_avg_winner_loser():
    """Avg winner/loser calculations."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [2.0, 4.0, -1.0, -3.0],  # avg_win=3.0, avg_lose=-2.0
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.avg_winner == 3.0
    assert metrics.avg_loser == -2.0

def test_metrics_rr_ratio():
    """R:R ratio calculation."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [2.0, 4.0, -1.0, -3.0],  # R:R = 3.0 / 2.0 = 1.5
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.rr_ratio == pytest.approx(1.5)

def test_metrics_ev():
    """EV calculation."""
    calc = MetricsCalculator()
    # 60% win rate, avg_win=2.0, avg_lose=-1.0
    # EV = (0.6 * 2.0) + (0.4 * -1.0) = 1.2 - 0.4 = 0.8
    df = pd.DataFrame({
        "gain_pct": [2.0, 2.0, 2.0, -1.0, -1.0],  # 3 wins, 2 losses
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.ev == pytest.approx(0.8)

def test_metrics_kelly():
    """Kelly criterion calculation."""
    calc = MetricsCalculator()
    # 60% win rate, R:R = 2.0
    # Kelly = 0.6 - (0.4 / 2.0) = 0.6 - 0.2 = 0.4 (40%)
    df = pd.DataFrame({
        "gain_pct": [2.0, 2.0, 2.0, -1.0, -1.0],
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.kelly == pytest.approx(40.0)

def test_metrics_empty_dataframe():
    """Empty DataFrame returns empty metrics."""
    calc = MetricsCalculator()
    df = pd.DataFrame(columns=["gain_pct"])
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.num_trades == 0
    assert metrics.win_rate is None

def test_metrics_no_winners():
    """All losers edge case."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [-1.0, -2.0, -3.0],
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.win_rate == 0.0
    assert metrics.avg_winner is None
    assert metrics.rr_ratio is None

def test_metrics_no_losers():
    """All winners edge case."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [1.0, 2.0, 3.0],
    })
    metrics = calc.calculate(df, "gain_pct", derived=True)
    assert metrics.win_rate == 100.0
    assert metrics.avg_loser is None
    assert metrics.rr_ratio is None

def test_metrics_breakeven_is_win():
    """Breakeven classified as win when breakeven_is_win=True."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [0.0, 1.0, -1.0],  # 0 is win
    })
    metrics = calc.calculate(df, "gain_pct", derived=True, breakeven_is_win=True)
    assert metrics.winner_count == 2  # 0.0 and 1.0
    assert metrics.loser_count == 1

def test_metrics_explicit_win_loss_column():
    """Use explicit win/loss column."""
    calc = MetricsCalculator()
    df = pd.DataFrame({
        "gain_pct": [2.0, -1.0, 3.0],
        "result": ["W", "L", "Win"],
    })
    metrics = calc.calculate(df, "gain_pct", win_loss_col="result")
    assert metrics.winner_count == 2
    assert metrics.loser_count == 1

@pytest.mark.slow
def test_metrics_performance_100k():
    """Performance: < 100ms for 100k rows."""
    import numpy as np
    from time import perf_counter
    
    calc = MetricsCalculator()
    np.random.seed(42)
    df = pd.DataFrame({
        "gain_pct": np.random.normal(0.5, 3, 100_000),
    })
    
    start = perf_counter()
    metrics = calc.calculate(df, "gain_pct", derived=True)
    elapsed = perf_counter() - start
    
    assert elapsed < 0.1, f"Metrics calculation took {elapsed:.3f}s, exceeds 100ms limit"
    assert metrics.num_trades == 100_000
```

**Widget Tests** (`tests/widget/test_metric_card.py`):

```python
import pytest
from pytestqt.qtbot import QtBot
from src.ui.components.metric_card import MetricCard
from src.ui.constants import Colors

def test_metric_card_displays_label(qtbot: QtBot):
    """Card displays label correctly."""
    card = MetricCard(label="Win Rate")
    qtbot.addWidget(card)
    assert "Win Rate" in card._label_widget.text()

def test_metric_card_displays_value(qtbot: QtBot):
    """Card displays formatted value."""
    card = MetricCard(label="Win Rate")
    qtbot.addWidget(card)
    card.update_value(67.5, format_spec=".1f")
    assert "67.5" in card._value_widget.text()

def test_metric_card_positive_color(qtbot: QtBot):
    """Positive values use SIGNAL_CYAN."""
    card = MetricCard(label="EV")
    qtbot.addWidget(card)
    card.update_value(3.2)
    style = card._value_widget.styleSheet()
    assert Colors.SIGNAL_CYAN in style

def test_metric_card_negative_color(qtbot: QtBot):
    """Negative values use SIGNAL_CORAL."""
    card = MetricCard(label="Avg Loser")
    qtbot.addWidget(card)
    card.update_value(-2.5)
    style = card._value_widget.styleSheet()
    assert Colors.SIGNAL_CORAL in style

def test_metric_card_none_displays_dash(qtbot: QtBot):
    """None values display '—'."""
    card = MetricCard(label="R:R")
    qtbot.addWidget(card)
    card.update_value(None)
    assert "—" in card._value_widget.text()

def test_metric_card_integer_formatting(qtbot: QtBot):
    """Integer values use thousands separator."""
    card = MetricCard(label="Trades")
    qtbot.addWidget(card)
    card.update_value(12847)
    assert "12,847" in card._value_widget.text()
```

### Fixtures to Add to conftest.py

```python
# tests/conftest.py additions
import pandas as pd
import numpy as np
from src.core.models import TradingMetrics

@pytest.fixture
def sample_trading_metrics() -> TradingMetrics:
    """Sample TradingMetrics for testing."""
    return TradingMetrics(
        num_trades=100,
        win_rate=60.0,
        avg_winner=2.5,
        avg_loser=-1.25,
        rr_ratio=2.0,
        ev=0.875,
        kelly=40.0,
        winner_count=60,
        loser_count=40,
        winner_std=1.2,
        loser_std=0.8,
        winner_gains=[2.0, 2.5, 3.0],
        loser_gains=[-1.0, -1.5],
    )

@pytest.fixture
def sample_balanced_trades() -> pd.DataFrame:
    """Balanced wins/losses trading data."""
    return pd.DataFrame({
        "ticker": ["AAPL"] * 10,
        "date": pd.date_range("2024-01-01", periods=10).date,
        "time": pd.date_range("2024-01-01 09:30", periods=10, freq="h").time,
        "gain_pct": [2.0, -1.0, 3.0, -2.0, 1.5, -0.5, 2.5, -1.5, 1.0, -1.0],
    })
```

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2026-01-10 | 0.1 | Initial draft | SM Agent (Bob) |
| 2026-01-10 | 0.2 | Validation fixes: HERO size 48→56px per architecture, added __init__.py export subtask | PO Agent (Sarah) |
| 2026-01-11 | 1.1 | **RETROFIT REQUIRED**: Added AC 8-12 for efficiency/stop-loss adjustments per Sprint Change Proposal | PM Agent (John) |

## Retrofit Notes (2026-01-11)

**Action Required:** This story needs code updates to add efficiency and stop loss adjustment logic.

**New Requirements (AC 8-12):**
- User inputs panel for Stop Loss % (default 8%) and Efficiency % (default 5%)
- Calculate `stop_adjusted_gain_pct`: if `mae_pct > stop_loss` then `-stop_loss`, else `gain_pct`
- Calculate `efficiency_adjusted_gain_pct = stop_adjusted_gain_pct - efficiency`
- All metrics use `efficiency_adjusted_gain_pct`
- User input changes trigger recalculation with 300ms debounce

**New Tasks Added:** Tasks 12-16 (see Tasks section above)

**Dependencies:**
- Story 1.4 retrofit must be completed first (mae_pct column mapping)

**Reference:** `docs/sprint-change-proposal-efficiency-stoploss.md`

---

## Dev Agent Record

### Agent Model Used

Claude Opus 4.5 (claude-opus-4-5-20251101)

### Debug Log References

No debug issues encountered.

### Completion Notes List

- All 11 tasks completed successfully
- TradingMetrics dataclass added to src/core/models.py with 7 core metrics + distribution data
- MetricsCalculator class created in src/core/metrics.py with vectorized pandas operations
- MetricCard widget created with HERO/STANDARD/COMPACT variants and color coding
- MetricsPanel widget added to data_input.py displaying 7 metric cards in grid layout
- Metrics calculation integrated into DataInputTab._on_mapping_continue() workflow
- AppState type hints updated from `object` to `TradingMetrics | None`
- All tests passing (162 tests)
- Linting passes (ruff check)
- Type checking passes (mypy)
- Performance test passes (<100ms for 100k rows)

**Retrofit Completion Notes (2026-01-11):**
- Tasks 12-16 completed successfully for stop loss and efficiency adjustments
- AdjustmentParams dataclass added with calculate_adjusted_gain() and vectorized calculate_adjusted_gains()
- AdjustmentInputsPanel widget with Stop Loss % and Efficiency % spinboxes
- MetricsCalculator.calculate() extended to accept adjustment_params and mae_col
- DataInputTab wired with 300ms debounce QTimer for adjustment changes
- AppState updated with adjustment_params attribute and adjustment_params_changed signal
- All 424 tests passing (409 when excluding flaky performance tests)
- Linting passes (ruff check)
- Type checking passes (mypy)

### File List

| File | Action |
|------|--------|
| src/core/models.py | MODIFIED - Added TradingMetrics dataclass, AdjustmentParams dataclass |
| src/core/metrics.py | NEW - MetricsCalculator class with adjustment support |
| src/core/app_state.py | MODIFIED - Updated type hints, added adjustment_params and signal |
| src/ui/components/metric_card.py | NEW - MetricCard widget |
| src/ui/components/__init__.py | MODIFIED - Export MetricCard |
| src/tabs/data_input.py | MODIFIED - Added MetricsPanel, AdjustmentInputsPanel, metrics integration with debounce |
| tests/unit/test_models.py | MODIFIED - Added TradingMetrics tests, TestAdjustmentParams tests |
| tests/unit/test_metrics.py | NEW - MetricsCalculator unit tests, TestMetricsCalculatorWithAdjustments |
| tests/widget/test_metric_card.py | NEW - MetricCard widget tests |
| tests/widget/test_metrics_panel.py | NEW - MetricsPanel widget tests |
| tests/widget/test_adjustment_inputs_panel.py | NEW - AdjustmentInputsPanel widget tests |
| tests/widget/test_data_input_tab.py | MODIFIED - Added TestDataInputTabAdjustmentPanel tests |

---

## QA Results

### Review Date: 2026-01-10

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

**Excellent implementation.** The story demonstrates high-quality code with proper separation of concerns:

- `TradingMetrics` dataclass in `src/core/models.py` is well-designed with comprehensive type hints and a factory method for empty datasets
- `MetricsCalculator` in `src/core/metrics.py` uses vectorized pandas operations for performance, properly handles all edge cases, and follows the logging pattern
- `MetricCard` widget is cleanly implemented with variant support (HERO/STANDARD/COMPACT) and proper color coding
- `MetricsPanel` correctly integrates with the data flow and handles all formatting requirements
- Integration in `DataInputTab` properly connects metrics calculation to the workflow

### Refactoring Performed

No refactoring required - implementation is clean and well-structured.

### Compliance Check

- Coding Standards: ✓ All type hints present, proper naming conventions, line length within limits
- Project Structure: ✓ Files in correct locations per architecture spec
- Testing Strategy: ✓ Comprehensive unit and widget tests with proper fixtures
- All ACs Met: ✓ All 7 acceptance criteria fully implemented and tested

### Improvements Checklist

[No outstanding items - implementation is complete]

- [x] 7 core metrics correctly calculated (num_trades, win_rate, avg_winner, avg_loser, rr_ratio, ev, kelly)
- [x] Win/Loss classification supports both explicit column and derived from gain %
- [x] Distribution data populated (winner_gains, loser_gains, std deviations)
- [x] MetricCard displays with proper formatting and color coding
- [x] MetricsPanel shows all 7 cards in grid layout
- [x] Edge cases handled (empty df, no winners, no losers)
- [x] Performance verified at <100ms for 100k rows

### Security Review

No security concerns - this story handles read-only metrics calculation with no external input validation required.

### Performance Considerations

Performance requirement met: The `test_performance_100k_rows` test verifies calculation completes in <100ms for 100,000 rows. Implementation uses vectorized pandas operations throughout.

### Files Modified During Review

None - no modifications made during QA review.

### Requirements Traceability

| AC | Test Coverage |
|----|---------------|
| AC1: 7 core metrics | `test_basic_calculation`, `test_win_rate_calculation`, `test_avg_winner_loser`, `test_rr_ratio`, `test_ev_calculation`, `test_kelly_calculation` |
| AC2: Win/Loss mapping | `test_explicit_win_loss_column`, `test_explicit_win_loss_with_integers`, `test_breakeven_is_win_true`, `test_breakeven_is_win_false` |
| AC3: Distribution data | `test_distribution_data`, `test_standard_deviation`, `test_single_winner_no_std` |
| AC4: Styled cards | `test_card_displays_label`, `test_card_displays_value`, `test_panel_displays_all_seven_cards`, all formatting tests |
| AC5: Color coding | `test_positive_color`, `test_negative_color`, `test_zero_color`, `test_none_uses_primary_color` |
| AC6: Edge cases | `test_empty_dataframe`, `test_no_winners`, `test_no_losers`, `test_none_values_display_dash` |
| AC7: Performance | `test_performance_100k_rows` (marked @pytest.mark.slow) |

### Test Summary

- **Total Tests**: 48 story-specific tests (162 total in suite)
- **Unit Tests**: 18 tests for MetricsCalculator
- **Widget Tests**: 17 tests for MetricCard, 13 tests for MetricsPanel
- **All Tests Pass**: ✓
- **Lint (ruff)**: ✓ No issues
- **Type Check (mypy)**: ✓ No issues found in 23 source files

### Gate Status

Gate: **PASS** → docs/qa/gates/1.6-core-metrics-calculation-display.yml

### Recommended Status

✓ Ready for Done - All acceptance criteria met, comprehensive test coverage, all quality checks pass.

---

### Retrofit Review Date: 2026-01-11

### Reviewed By: Quinn (Test Architect)

### Retrofit Code Quality Assessment

**Overall: Excellent** - The efficiency and stop loss adjustment retrofit was implemented with high quality.

**Verified Implementations:**

1. **AdjustmentParams Dataclass** (models.py:131-175)
   - ✓ Fields: `stop_loss: float = 8.0`, `efficiency: float = 5.0`
   - ✓ `calculate_adjusted_gain()` method for single trade
   - ✓ `calculate_adjusted_gains()` vectorized method using `pd.Series.where()`
   - ✓ Proper docstrings and type hints

2. **AdjustmentInputsPanel Widget** (data_input.py:697-755)
   - ✓ Stop Loss % spinner (0-100, default 8, step 0.5)
   - ✓ Efficiency % spinner (0-100, default 5, step 0.5)
   - ✓ `params_changed` signal emission
   - ✓ `get_params()` and `set_params()` methods
   - ✓ Signal blocking during programmatic updates

3. **MetricsCalculator Integration** (metrics.py:26-55)
   - ✓ Accepts `adjustment_params: AdjustmentParams | None`
   - ✓ Accepts `mae_col: str | None`
   - ✓ Calculates adjusted gains before metric computation
   - ✓ Falls back to original gains when params not provided

4. **DataInputTab Wiring** (data_input.py:1216-1590)
   - ✓ `AdjustmentInputsPanel` integrated into UI
   - ✓ 300ms debounce timer using `QTimer.setSingleShot(True)`
   - ✓ `_on_adjustment_params_changed()` handler
   - ✓ `_recalculate_metrics()` triggered on debounce timeout
   - ✓ AppState updated with `adjustment_params` and signal

5. **AppState Integration** (app_state.py:39, 59)
   - ✓ `adjustment_params_changed` signal
   - ✓ `adjustment_params: AdjustmentParams | None` attribute

### Retrofit Acceptance Criteria Traceability

| AC | Description | Implementation | Test Coverage |
|----|-------------|----------------|---------------|
| AC8 | User inputs panel | `AdjustmentInputsPanel` widget | 11 tests in test_adjustment_inputs_panel.py |
| AC9 | stop_adjusted_gain_pct calculation | `AdjustmentParams.calculate_adjusted_gain()` | test_calculate_adjusted_gain_* (4 tests) |
| AC10 | efficiency_adjusted_gain_pct calculation | Subtraction in calculate methods | test_adjustment_params_applied, test_adjustment_avg_winner_loser |
| AC11 | All metrics use adjusted gains | `MetricsCalculator.calculate()` uses adjusted gains | TestMetricsCalculatorWithAdjustments (6 tests) |
| AC12 | Recalculation with 300ms debounce | `QTimer` in `DataInputTab` | test_adjustment_debounce_timer_exists |

### Retrofit Test Verification

| Test File | Tests | Status |
|-----------|-------|--------|
| test_metrics.py | 23 tests (6 new adjustment tests) | ✓ All pass |
| test_models.py (AdjustmentParams) | 8 tests | ✓ All pass |
| test_adjustment_inputs_panel.py | 11 tests (NEW) | ✓ All pass |
| test_data_input_tab.py | 12 tests (4 new) | ✓ All pass |
| test_metrics_panel.py | 13 tests | ✓ All pass |
| **Total Retrofit Tests** | **59 tests** | **✓ PASS** |

### Retrofit Compliance Check

- Coding Standards: ✓ Lint passes (ruff)
- Type Safety: ✓ Type check passes (mypy)
- Performance: ✓ Vectorized operations in `calculate_adjusted_gains()`
- UI/UX: ✓ Debounce prevents excessive recalculation
- All Retrofit ACs Met: ✓ AC8-AC12 fully implemented

### Retrofit Security Review

No security concerns:
- User input is bounded by QDoubleSpinBox (0-100 range)
- No file system or network operations
- Values used for internal calculations only

### Retrofit Performance Considerations

- Vectorized adjustment calculation using `pd.Series.where()` - O(n)
- 300ms debounce prevents UI lag during rapid input changes
- Performance test still passes (<100ms for 100k rows)

### Retrofit Gate Status

Gate: **PASS** → docs/qa/gates/1.6-core-metrics-calculation-display.yml (updated)

### Retrofit Recommended Status

✓ **Ready for Done** - All 12 acceptance criteria (original 7 + retrofit AC8-12) met with comprehensive test coverage.
